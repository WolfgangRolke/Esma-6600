---
header-includes: \usepackage{color}
                 \usepackage{float}
                 \usepackage{graphicx}
output:
  pdf_document:
    fig_caption: no
  html_document: default
---

```{r, echo=FALSE, warning=FALSE, message=FALSE}
source("../R/setup.rmd.R", local=TRUE)
setup.rmd(local.env=environment())
```
`r hl()$basefontsize()`
`r hl()$style()`

## Random Variables and Distribution Functions 

**Definition**

A *random variable* (r.v.) X is set-valued function from the sample space into $\mathbb{R}$. For any set of real numbers $A \in \mathbb{R}$ we define the probability 

$$P(X \in A) = P(X^{-1}(A))$$

where $X^{-1}(A)$ is the set of all points in S such that X maps the points into A.

#### **Example** 

Say we flip a fair coin three times. Let X be the number of "heads" in these three flips.
    
Now S=(\{H,H,H\}, (H,H,T), .., (T,T,T)\}. 

X maps S into $\mathbb{R}$, for example X(\{(H,H,H)\})=3 and X(\{(H,H,T)\})=2. 

What is P(X=2)?
 
P(X=2) =  
P(X^-1^(2)) =  
P( all the outcomes in S that are mapped onto 2 ) =  
P({(H,H,T), (H,T,H), (T,H,H)} =  3/8 

`r hl()$hr()`

There are two basic types of r.v.'s:

-  If X takes countably many values, X is called a **discrete** r.v. 
-  If X takes uncountably many values, X is called a **continuous** r.v.

#### **Example** 

Consider the following experiment: we randomly select a point in the interval [A,B] for some A<B. We allow all points in [A,B], so X takes uncountably many values, and therefore is a continuous random variable. By "randomly" we mean that the probability for a chosen point to be in some  interval depends only on the length of the interval. Let X be the point chosen. Clearly

1 = P(A<X<B) 

Let A<a<b<B. Now the interval (a,b) has length b-a, the interval (A,B) has length B-A, and we have

a-b = [(b-a)/(B-A)]*(B-A) = c(B-A) 

where c=[(b-a)/(B-A)] 

therefore 

P(a<X<b) = cP(A<X<B) = c*1 = (b-a)/(B-A) 

This is a standard random variable. We often use the following notation:

$X \sim U[A,B]$

`r hl()$hr()`

There are some technical difficulties when defining a r.v. on a sample space like $\mathbb{R}$, it turns out to be impossible to define it for every subset of $\mathbb{R}$ without getting logical contradictions. The solution is to define a *$\sigma$-algebra* on the sample space and then define X only on that $\sigma$-algebra. 

The most commonly used $\sigma$-algebra is the Borel $\sigma$-algebra, which is the union and intersection of all intervals of the type (a, b), (a, b], [a, b) where a and b can be $\pm \infty$. All of this belongs to the brach of mathematics called *measure theory*. In what follows we will ignore these technical difficulties.

#### **Example** 

In the example of the uniform random variable above we defined probabilities only for intervals. It turns out that this is all that is needed. In fact the set of all unions and intersections of intervals forms a $\sigma$-algebra on the real line. There are however also sets on the real line that can not be expressed as the union and intersection of intervals. For those probabilities are then not defined. 

`r hl()$hr()`

Almost everything to do with r.v.'s has to be done twice, once for discrete and once for continuous r.v.'s. This separation is only artificial, it goes away once a more general definition of "integral" is used (Riemann-Stilties or Lebesgue).

### (Cumulative) Distribution Function

**Definition**

The (cumulative) distribution function (cdf) of a r.v. X is defined by 

$$F(x)=P(X \le x) \text{ for all } x \in \mathbb{R}$$

#### **Example** 

$X \sim U[A,B]$

x<A: F(x) = P(X<x) = 0  
A<x<B: F(x) = P(X<x) = P(A<X<x) = (x-A)/(B-A)  
x>B: F(x) = P(X<x) = 1  

**Theorem**

Let F be the cdf of some random variable X. Then 
  
1. cdf's are standard functions on $\mathbb{R}$

2. $0 \le F(x) \le 1$

3. cdf's are non-decreasing

4. cdf's are right-continuous

5. 
$$\lim_{x \rightarrow - \infty} F(x) = 0$$  
$$\lim_{x \rightarrow  \infty} F(x) = 1$$

**proof**

1.  probabilities are unique so $F(x) = P(X \le x) = P(\{\omega \in S:X(\omega) \le x\})$ is unique

2.  $0  \le  P(X \le x) = P(\{\omega  \in S:X(\omega) \le x\})  \le  1$ from axiom 1

3) say x<y, then 

$\{\omega  \in S:X(\omega) \le x\} \subset \{\omega  \in S:X(\omega ) \le y\}$

and so 

$F(x) = P(X \le x)  \le  P(X \le y) = F(y)$

4) a function F is right continuous if

![](graphs/prob339.png)

the limit is the same as the intersection because   if h<k A~x,h~ $\subset$ A~x,k~, and the intersection is the empty set otherwise there exists y such y $\in$ A~x,h~  for all h, that is $x<y \le x+h$, a contradiction. 

5) similar to 4.

**Theorem**

Let F be function that is increasing, right-continuous and has $0 \le F(x) \le 1$, then there exists a random variable that has F as its cdf.

**proof**: too deep for us 

**Theorem**

Let F be the cdf of a rv X. Then F has at most countably many points of discontinuity.

**proof**

F is increasing so any point of discontinuity is a jump point up. Let A~n~  be the set of all point where F jumps up by more then 1/n. Then |A~n~|<n because 0<F<1. Let A be the set of all jump points of F, then 

$A= \cup A_n$ 

and therefore A is countable.

Note another consequence of this proof: for any $\epsilon >0$ there are at most finitely many points where F jumps up by more then $\epsilon$. 


##### **Example**   

We roll a fair die until the first "6". Let the rv X be the number of rolls. Find the cdf F of X.

Solution: note $X \in \{1,2,3,...\}$

let A~i~  be the event "a six on the i^th^ roll", i=1,2,3, .... Then

$$
\begin{aligned}
&P(X=k)   = P(A_1^c \cap A_2^c \cap ..\cap A_{k-1}^c \cap A_k) =\\
&P(A_1^c)P(A_2^c ) ..P(A_{k-1}^c )P( A_k)    = (\frac56)^{k-1}\frac16 \\
&P(X \le k) = \sum_{i=1}^k P(X=k) = \\
&\sum_{i=1}^k (\frac56)^{i-1}\frac16 = \\
&\sum_{j=0}^{k-1} (\frac56)^j\frac16 = \\
&\frac16 \frac{1-(5/6)^{(k-1)+1}}{1-5/6} = 1-(5/6)^k \\
\end{aligned}
$$



 so for $k \le x<k+1$ we have F(x)=1-(5/6)^k^
 
![](graphs/prob345.png)
Notice that the cdf is a step function. This is always the case for a discrete random variable. 

### Probability Density Function

The probability density function (pdf) of a discrete r.v. X is defined by 

$f(x) = P(X=x)$

#### **Example**  

the pdf of X in the example above is given by f(x) = 1/6*(5/6)^x-1^ if $x \in \{1,2,..\}$, 0 otherwise.


Note that it follows from the definition and the axioms that for any pdf f we have

$$
\begin{aligned}
&f(x) \ge 0 \\
&\sum_x f(x)   = 1
\end{aligned}
$$

#### **Example** 

Say f(x)=c/x^2^, x=1,2,3,... is a pdf. Find c

$1=\sum_x f(x) = \sum_{i=1}^\infty c/i^2 = c\pi^2/6$

so $c=6/\pi^2$.

#### **Example** 

say we have a coin that comes up heads with probability p. Let X be the number of heads in n flips of the coin. Find the pdf of X.

Let's start with a couple of small n's:

n=1: 

S=\{H,T\}   
P(X=0)=P(T)=1-p P(X=1)=P(H)=p

n=2: 

S=\{(H,H),(H,T),(T,H),(T,T)\}  
P(X=0) = P((T,T))=P(T)P(T) = (1-p)^2^  
P(X=1) = P(\{(H,T),(T,H)\}) = P(H)P(T)+P(T)P(H) = 2p(1-p)  
P(X=2)=P((H,H))=P(H)P(H)=p^2^

n=3: 

S=\{(H,H,H),(H,H,T),(H,T,H),(H,T,T),(T,H,H),(T,H,T),(T,T,H),(T,T,T)\}  
P(X=0) = P((T,T,T))=P(T)P(T)P(T)=(1-p)^3^  
P(X=1) = P(\{(H,T,T),(T,H,T),(T,T,H)\}) = 3P(H)P(T)P(T) = 3p(1-p)^2^  
P(\{(H,T,T),(H,T,H),(T,H,H)\}) = 3P(H)P(H)P(T) = 3p^2^(1-p)  
P(X=3)=P((H,H,H))=P(H)P(H)P(H)=p^3^
 
apparently for some n we have something like

$P(X=k)=c_{n,k} p^k(1-p)^{n-k}$

what is c_n,k ? From the first few cases one might guess 

$c_{n,k}={{n}\choose{k}}$

Let's verify this using an induction proof. For this we will use the law of total probability, conditioning on whether the last flip was a heads or a tails. Also, Let X~n~  be X if we flip the coin n times. Now


![](graphs/prob341.png)

**Definition**

f is the pdf of the continuous random variable X iff

$F(x) = \int_{- \infty}^x f(t dt)$

or (if the cdf is differentiable at x)

$f(x)=F'(x)$

Again it follows from the definition and the axioms that for any pdf f we have

$$
\begin{aligned}
&f(x) \ge 0 \\
&\int_{-\infty}^\infty f(x) dx   = 1
\end{aligned}
$$

#### **Example** 

$X \sim U[A,B]$

$x<A$ or $x>B$: f(x)=F'(x) =  0  
$A<x<B$: f(x)=F'(x) = d/dx[ (x-A)/(B-A)] = 1/(B-A)  
at x=A and x=B f is not defined because F is not differentiable 

#### **Example**: 

Show that $f(x)= \lambda \exp(-\lambda x)$ if x>0, 0 otherwise defines a pdf, where $\lambda>0$.

Solution: clearly $f(x) \ge 0$ for all x.

$$
\begin{aligned}
&\int_{-\infty}^\infty f(x) dx     = \\
&\int_{0}^\infty \lambda \exp(-\lambda x) dx     = \\
&- \exp(-\lambda x) |_0^\infty    = 0-(-1) = 1\\
\end{aligned}
$$

This r.v. X is called an *exponential* r.v. with rate $\lambda$. We often write $X \sim \text{Exp}(\lambda)$.

#### **Example** 

Say f(x)=c/x^2^, x>1 is a pdf. Find c.

![](graphs/prob312.png)

#### **Example** 

Say $f(x)=cx\sin(\pi x)$, $0 \le x \le 1$, 0 otherwise, is a pdf. Find c.

![](graphs/prob313.png)

#### **Example** 

Say f(x)=cexp{-x^2^} is a pdf. Find c.

Unfortunately f does not have an anti-derivative, so this is tricky problem. Using Numerical integration one can show that c=0.8547. 

`r hl()$hr()`

Although we usually deal with random variables that are either discrete or continuous, in real life they can be mixed: 

#### **Example** 

Consider the following experiment: first we flip a fair coin. If the coin comes up heads we roll a fair die and X is the number on the die, otherwise we choose $X \sim U[1,6]$. Find the cdf of X

![](graphs/prob347.png)

![](graphs/prob349.png)

![](graphs/prob346.png) 

#### **Example** (Simple Random Walk) 

Let X~0~ =0, and P(X~n~ =1)=p, P(X~n~ =-1)=q=1-p

let $S_n  = \sum_{k=0}^n X_k$  and define the event $A_n  = \{S_n =0\}$.

We want to find $P(\{A_n  \text{ i.o. }\})$. By Kolmogorov's 0-1 law we know that it is either 0 or 1. But which is it?

Let $P_{00}^{(n)} = P(A_n )$, then

![](graphs/mark145.png)

and by the Borel-Cantelli lemmas we see that $P(\{A_n  \text{ i.o. }\})=1$ if p=1/2 and 0 otherwise. 
