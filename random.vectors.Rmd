---
header-includes: \usepackage{color}
                 \usepackage{float}
output:
  pdf_document:
    fig_caption: no
  html_document: default
---

```{r, echo=FALSE, warning=FALSE, message=FALSE}
source("R/setup.rmd.R", local=TRUE)
setup.rmd(local.env=environment())
```
`r hl()$basefontsize()`
`r hl()$style()`

## Random Vectors 

A random vector is a multi-dimensional random variable.

#### **Example 1** 

we roll a fair die twice. Let X be the sum of the rolls and let Y be the absolute difference between the two rolls. Then (X,Y) is a 2-dimensional random vector. The joint pdf of (X,Y) is given by:

```{r echo=FALSE}
x <- sample(1:6, size=1e5, replace = TRUE)
y <- sample(1:6, size=1e5, replace = TRUE)
z <- round(table(x+y, abs(x-y))/1e5*36)
kable.nice(z)
```

where every number is divided by 36.

`r hl()$hr()`

all definitions are straightforward extensions of the one-dimensional case.

#### **Example**  

for a discrete random vector we have the pdf f(x,y) = P(X=x,Y=y)

Say f(4,0) = P(X=4, Y=0) = P(\{(2,2)\}) = 1/36 or f(7,1) = P(X=7,Y=1) = P(\{(3,4),(4,3)\}) = 1/18

**Example** 

Say f(x,y)=cxy is a pdf with $x \in \{1,2,3\}$ and $y \in \{0,2\}$. Find c.

$1=\sum_{x,y}  f(x,y) =$  
$f(1,0)+f(1,2)+f(2,0)+f(2,2)+f(3,0)+f(3,2) =$   
$c(1  \times 0+1  \times 2+2  \times 0+2  \times 2+3  \times 0+3  \times 2) = 12c$ so $c=1/12$

#### **Example** 

Say $f(x,y)=cxy, 0  \le  x,y  \le  1$ is a pdf. Find c.

![](graphs/prob314.png)

so c=4.

#### **Example** 

Say $f(x,y)=cxy, 0  \le  x<y  \le  1$ is a pdf. Find c.

![](graphs/prob315.png)

so c=8.

#### **Example** 

Say (X,Y) is a discrete rv with joint pdf $f(x,y)=cp^x, x,y \in \{0,1,..\}, y  \le  x, 0<p<1$. Find c

![](graphs/prob328.png)

#### **Example** 

Say (X,Y,Z) is a continuous rv with f(x,y,z) = c(x+y)z if $0<x,y,z<1$ and 0 otherwise. Find c

![](graphs/prob334.png)

so c=2

#### **Example** 

Let's extend the idea of a uniform random variable to two dimensions. To begin, let's start with the unit square [0,1]~2~.

Again, the idea of uniform is taken to mean that the probability of a point (X,Y) being in some area is proportional to the size of the area. Therefore if A is some area in [0,1]~2~ we have

$P((X,Y)  \in  A) = \text{area}(A)$

say $0<x,y<1$, then

$F(x,y) = P(X<x,Y<y) = \text{area}([0,x]*[0,y]) = xy$

$f(x,y) = d^2/dxdy F(x,y) =d/dx[d/dy(xy)] = d/dx[x] = 1$

Now say (X,Y) is uniform on $\{(x,y\}: 0<x<y^\alpha  <1\}$ for some $\alpha  >0$. Find the joint pdf of (X,Y).

First we need the total area:
  
![](graphs/prob342.png) 

so $f(x,y)=\alpha+1$ if $0<x<y^\alpha  <1$.  
  
 
### Marginal Distributions

**Definition**

Say (X,Y) is a discrete (continuous) r.v. with joint pdf (pdf) f. Then the *marginal* pdf f~X~  is given by
  
![](graphs/prob38.png)

#### **Example** 

Say X is the sum and Y is the absolute difference of two dice. If we add the row and column totals to the table above we get 

```{r echo=FALSE}
x <- sample(1:6, size=1e5, replace = TRUE)
y <- sample(1:6, size=1e5, replace = TRUE)
z <- round(table(x+y, abs(x-y))/1e5*36)
z <- cbind(z, apply(z, 1, sum))
z <- rbind(z, apply(z, 2, sum))
colnames(z)[7]<- "X"
rownames(z)[12]<- "Y"
kable.nice(z)
```

and these are the marginals. For example we find f~X~ (2) = 1/36 or f~Y~ (3) = 6/36.

#### **Example** 

Say (X,Y) is a rv with joint pdf f(x,y)=xy/12  with $x \in \{1,2,3\}$ and $y \in \{0,2\}$  Now

f~X~ (3) = f(3,0) + f(3,2)  = 
3x0x1/12 + 3x2x1/12 = 6/12 = 1/2 
  
f~Y~ (0) = f(1,0) + f(2,0) +  f(3,0) = 0

#### **Example** 

Say (X,Y) is a rv with joint pdf f(x,y)=8xy, $0 \le x<y \le 1$. Find f~Y~ (y)
   
![](graphs/prob316.png)

![](graphs/prob330.png)

Note that f_Y (y) is s proper pdf: $f_Y (y) \ge 0$ for all y and

![](graphs/prob317.png)

#### **Example** 

Say (X,Y,Z) is a continuous rv with f(x,y,z) = 2(x+y)z if $0<x,y,z<1$ and 0 otherwise.

![](graphs/prob335.png)

### Conditional Random Variables

**Definition**

let (X,Y) be a r.v. with joint pdf f(x,y) and marginal  f~Y~ . For any y such that f~Y~ (y)>0 the *conditional pdf* of X|Y=y is defined by
  
$$f_{X|Y=y}(x|y) = \frac{f(x,y)}{f_Y(y)}$$

Note that a conditional pdf requires a specification for a value of the random variable on which we condition, something like f_X|Y=y . An expression like f_X|Y  is not defined!

Note that is is exactly the same as the definition for conditional probabilities of events. For example if (X,Y) is a discrete rv, then

![](graphs/prob331.png)

#### **Example**

Say X is the sum and Y is the absolute difference of two dice.  Find f~X|Y=5~ (7|5) and f~Y|X=3~ (7|3)
```{r echo=FALSE}
kable.nice(z)
```


![](graphs/prob310.png)

#### **Example** 

f(x,y)=8xy, $0 \le x<y \le 1$. Find f~X|Y=y~(x|y)
  
f~X|Y=y~(x|y) = f(x,y)/f~Y~(y) = 8xy/4y^3^ = 2x/y^2^, 

for x, y with $\mathbf{0 \le x \le y}$. 

Here y is a fixed number! 
  
Again, note that a conditional pdf is a proper pdf:

![](graphs/prob318.png)

#### **Example** 

Say (X,Y,Z) is a continuous rv with f(x,y,z) = 2(x+y)z if $0<x,y,z<1$ and 0 otherwise. Then all the marginals are:

![](graphs/prob336.png)

#### **Example**

say $f(x,y)=\alpha+1$ if $0<x<y^\alpha<1$. Find the marginals and the conditional pdf's. Verify that they are proper pdf's.

![](graphs/prob343.png) 

### Independence

**Definition**

Two r.v. X and Y are said to be independent iff 

$$f_{X,Y} (x,y)=f_X (x)f_Y (y)$$

for all x,y 

Notation: we will use the notation $X \perp Y$ if X and Y are independent. 

#### **Example** 

Say X is the sum and Y is the absolute difference of two dice. Previously we found 

f~X,Y~ (7,1) = 1/18 

but 

f~X~ (7)f~Y~ (1) = 1/6 x 10/36 = 5/108

so X and Y are not independent

**Theorem**

say f(x,y) is the joint pdf of a random vector (X,Y). Then X and Y are independent if there exist functions g and h such that

f(x,y)=g(x)h(y)

**proof**

the only difference between the definition and the theorem is that g and h need not be proper densities. But  first of all we can assume that g and h are non-negative, otherwise just take |g| and |h|.

Moreover 

![](graphs/prob348.png)  

 where $0<c<\infty$, so g/c = f~X~  , and similarly h/d=f~Y~
 
#### **Example** 

say f(x,y)=exp(-x-y), x,y>0. Then 

f(x,y) = exp(-x-y) = exp(-x)exp(-y) = g(x)h(y)

so X and Y are independent.  

`r hl()$hr()`

Mostly the concept of independence is used in reverse: we assume X and Y are independent (based on good reason!) and then make use of the formula:


#### **Example** 
Say we use the computer to generate 10 independent exponential r.v's with rate $\lambda$. What is the probability density function of this random vector? 
  
We have $f_{X_i}(x_i )=\lambda \exp(-\lambda x_i )$  for i=1,2,..,10 so 
  
$$
\begin{aligned}
&f_{(X_1 ,..,X_{10} )} (x_1 , .., x_10 )    = \\
&\lambda \exp(- \lambda x_1 ) \times..\times  \lambda \exp(- \lambda x_10 )    = \\
&\lambda^{10}\exp(-\lambda(x_1 +..+x_{10} ))
\end{aligned}
$$

#### **Example** 

Say (X,Y)  is a discrete random vector with

```{r echo=FALSE}
df <- data.frame(x=c("1/10", "1/10", "1/10"),
                 y=c("1/10", "1/2", "1/10"))
colnames(df) <- 1:2
rownames(df) <- 1:3
df
kable.nice(df)
```
Find the conditional pdf of X|Y=y

f~X|Y=y~ (x|y)=f(x,y)/f~Y~ (y)

$f_Y (y) = \sum_x f(x,y)$

f~Y~ (1) = f(1,1)+f(2,1)+f(3,1)=3/10  
f~Y~ (2) = f(1,2)+f(2,2)+f(3,2)=7/10

so

f~X|Y=1~ (1|1)=f(1,1)/f~Y~ (1) = (1/10)/(3/10) = 1/3  
f~X|Y=1~ (2|1)=f(2,1)/f~Y~ (1) = (1/10)/(3/10) = 1/3  
f~X|Y=1~ (3|1)=f(3,1)/f~Y~ (1) = (1/10)/(3/10) = 1/3

so

![](graphs/prob319.png)

f~X|Y=2~ (1|2)=f(1,2)/f~Y~ (2) = (1/10)/(7/10) = 1/7  
f~X|Y=2~ (2|2)=f(2,2)/f~Y~ (2) = (1/2)/(7/10) = 5/7  
f~X|Y=2~ (3|2)=f(3,2)/f~Y~ (2) = (1/10)/(7/10) = 1/7

so

![](graphs/prob320.png)

#### **Example** 

Let the continuous random vector (X,Y) have joint density $f(x,y)=e^{-y}, 0<x<y<\infty$

Show that f is indeed a proper density

![](graphs/prob321.png)

![](graphs/prob332.png)

Find f~Y|X=x~ (y|x)

f~Y|X=x~ (y|x) = f(x,y)/f~X~ (x)

![](graphs/prob322.png)

so

![](graphs/prob323.png)

Show that f~Y|X=x~ (y|x) is also a proper density

![](graphs/prob324.png)

#### **Example**

We have a "device" which generates a random number Y according to an exponential distribution with rate $\lambda$. We don't know exactly what $\lambda$ is, but we do know that $\lambda=x$ with probability 0.5^x^ where x=1,2,3,... Find the pdf of Y. Verify that your answer is a proper pdf.

We have a discrete r.v X with pdf 

f~X~ (x)=0.5^x^, x=1,2,.. 

and a conditional rv Y with pdf 

f~Y|X=x~ (y|x)=xexp(-xy), y>0

We want f~Y~ (y). It turns out that if we are dealing with a continuous rv. it is often better to first find the cdf $F_Y (y)=P(Y \le y)$. Now  first we have

![](graphs/prob325.png)
  
a little bit of care: the geometric series $\sum q^k$ only converges if |q|<1. Here y>0, so e^y^>1 so $1/2e^y < 0.5 < 1$, we are save.

Now

![](graphs/prob333.png)

This type of model is called a *hierarchical model*, with one rv defined conditional on another. This way of describing a model is very useful in real live.

### Law of Total Probability

We have previously seen the law of total probability for events. There are corresponding versions for random variables:

-  **Discrete-Discrete**

Say X and Y are discrete rv's with pdf's f~X~  and f~Y~ , respectively. Let B=\{X=x\} and A~y~ =\{Y=y\}.

Then $\{A_y , y \in S\}$ forms a partition and we have

$$
\begin{aligned}
&f_X (x) = P(X=x) = P(B)    = \\
&\sum_y P(B|A_y )P(A_y )    = \\
&\sum_y f_{X|Y=y} (x|y)f_Y (y)    
\end{aligned}
$$
  

-  **Discrete-Continuous**

Say X is a discrete rv with pdf f~X~  and Y is a continuous rv with pdf f~Y~.

Here we need to be careful: for a discrete rv f~X~ (x)=P(X=x) makes sense, but for a continuous one we have

$$
\begin{aligned}
&P(Y=y) = \lim_{h \rightarrow 0} P(y \le Y \le y+h)    = \\
&\lim_{h \rightarrow 0}  \int_y^{y+h} f_Y (t)dt = \\
&\lim_{h \rightarrow 0}  (F_Y (y+h)-F_Y (y)) = 0
\end{aligned}
$$
**for all y**!

first we condition on the discrete rv.: Now the event B=\{Y=y\} does not work because P(B)=0 for all y. Let's instead consider the event $B=\{Y \le y\}$:

![](graphs/prob326.png)

For conditioning on the continuous rv we need to define a new discrete rv Y' with 

$Y'= ih \text{ if }ih \le Y<(i+1)h$

Then

![](graphs/prob327.png)

because this is a Riemann sum, so it converges to the corresponding integral.

-  **Continuous-Continuous**

Actually, same as above, the same proof works for this case as well!

#### **Example** 

back to the example above with the "device". Now we have the following solution: 

![](graphs/prob337.png)

#### **Example** 

again the example above with the "device", but now the rate X has a uniform distribution on [0,1], that is f~X~ (x)=1 if $0<x<1$. Then:

![](graphs/prob338.png)